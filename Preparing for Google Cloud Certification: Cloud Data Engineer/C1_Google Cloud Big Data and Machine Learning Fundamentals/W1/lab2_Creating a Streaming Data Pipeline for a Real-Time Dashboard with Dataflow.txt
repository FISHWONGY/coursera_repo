***
Task 1. Source a pre-created Pub/Sub topic and create a BigQuery dataset

In this task, you create the taxirides dataset. You have two different options which you can use to create this, using the Google Cloud Shell or the Google Cloud Console.

Pub/Sub is an asynchronous global messaging service. By decoupling senders and receivers, it allows for secure and highly available communication between independently written applications. Pub/Sub delivers low-latency, durable messaging.

In Pub/Sub, publisher applications and subscriber applications connect with one another through the use of a shared string called a topic. A publisher application creates and sends messages to a topic. Subscriber applications create a subscription to a topic to receive messages from it.

Google maintains a few public Pub/Sub streaming data topics for labs like this one. We'll be using an extract of the NYC Taxi & Limousine Commissionâ€™s open dataset. The Pub/Sub topic has already been created and populated in your project environment.

BigQuery is a serverless data warehouse. Tables in BigQuery are organized into datasets. In this lab, messages published into Pub/Sub will be aggregated and stored in BigQuery.

Use one of the following options to create a new BigQuery dataset:

1. In Cloud Shell (Cloud Shell icon), run the following command to create the taxirides dataset.
```bq --location=us-west1 mk taxirides```

```bq mk taxirides```

2. Run this command to create the taxirides.realtime table (empty schema that you will stream into later).
```
bq --location=us-west1 mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime
```

```
bq mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime
```

Option 2: The BigQuery Console UI
Note: Skip these steps if you created the tables using the command line.

    1. In the Google Cloud console, in the Navigation menu(Navigation Menu), click BigQuery.

    2. If you see the Welcome dialog, click Done.

    3. Click on View actions (View Actions) next to your Project ID, and then click Create dataset.

    4. In Dataset ID, type taxirides.

    5. In Data location, click us-west1 (Oregon), and then click Create Dataset.

    6. In the Explorer pane, click expand node (Expander) to reveal the new taxirides dataset.

    7. Click on View actions (View Actions) next to the taxirides dataset, and then click Open.

    8. Click Create Table.

    9. In Table, type realtime

    10. For the schema, click Edit as text and paste in the following:

    ```
    ride_id:string,
point_idx:integer,
latitude:float,
longitude:float,
timestamp:timestamp,
meter_reading:float,
meter_increment:float,
ride_status:string,
passenger_count:integer
```


    11. In Partition and cluster settings, select timestamp.

    12. Click Create Table.


***
Task 2. Create a Cloud Storage bucket

    1. In the Cloud console, in the Navigation menu (Navigation Menu), click Cloud Storage > Buckets.
    2. Click Create Bucket.
    3. For Name, copy and paste in your Project ID, and then click Continue.
    4. For Location type, click Multi-region if it is not already selected.
    5. Click Create.



***
Task 3. Set up a Dataflow Pipeline

In this task, you set up a streaming data pipeline to read sensor data from Pub/Sub, compute the maximum temperature within a time window, and write this out to BigQuery.

Dataflow is a serverless way to carry out data analysis.
Restart the connection to the Dataflow API.

    1. In the Cloud console, in the top search bar, type Dataflow API, and then press ENTER.

    2. In the search results window, click Dataflow API.

    3. click Manage.

    4. Click Disable API.

    5. In the Disable dialog, click Disable.

    6. Click Enable.

Create a new streaming pipeline:

    1. In the Cloud console, in the Navigation menu (Navigation Menu), click Dataflow.

    2. In the top menu bar, click Create Job From Template.

    3. Type streaming-taxi-pipeline as the Job name for your Dataflow job.

    4. In Regional endpoint, select us-central1 (Iowa).

    5. In Dataflow template, select the Pub/Sub Topic to BigQuery template.

    6. In Input Pub/Sub topic, select the topic that already exists in your project from the dropdown list . It will appear like the following:

       projects/<myprojectid>/topics/taxirides-realtime

    7. In BigQuery output table, type <myprojectid>:taxirides.realtime

    8. In Temporary location, click Browse.

    9. Click view child resources(view child resources).

    10. Click Create new folder(new folder), and then type the name tmp.

    11. Click Create, and then click Select.

    12. Click Show Optional Parameters.

    13. In Max workers, type 2

    14. In Number of workers, type 1

    15. Click Run Job.

A new streaming job has started! You can now see a visual representation of the data pipeline. It will take 3 to 5 minutes for data to begin moving into BigQuery.


***
Task 4. Analyze the taxi data using BigQuery

In this task, you analyze the data as it is streaming.

    1. In the Cloud console, in the Navigation menu (Navigation Menu), click BigQuery.

    2. If the Welcome dialog appears, click Done.

    3. In the Query Editor, type the following, and then click Run:
    ```
    SELECT * FROM taxirides.realtime LIMIT 10
    ```



***
Task 5. Perform aggregations on the stream for reporting

In this task, you calculate aggregations on the stream for reporting.

    1. In the Query Editor, clear the current query.

    2. Copy and paste the following query, and then click Run.

    ```
    WITH streaming_data AS (
SELECT
  timestamp,
  TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,
  TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,
  TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,
  ride_id,
  latitude,
  longitude,
  meter_reading,
  ride_status,
  passenger_count
FROM
  taxirides.realtime
ORDER BY timestamp DESC
LIMIT 1000
)
# calculate aggregations on stream for reporting:
SELECT
 ROW_NUMBER() OVER() AS dashboard_sort,
 minute,
 COUNT(DISTINCT ride_id) AS total_rides,
 SUM(meter_reading) AS total_revenue,
 SUM(passenger_count) AS total_passengers
FROM streaming_data
GROUP BY minute, timestamp
```

The result shows key metrics by the minute for every taxi drop-off.

    3. Click Save > Save query.

    4. In the Save query dialog, in the Name field, type My Saved Query.

    5. Click Save.



***
Task 6. Stop the Dataflow Job

In this task, you stop the Dataflow job to free up resources for your project.

    1. In the Cloud console, in the Navigation menu (Navigation Menu), click Dataflow.

    2. Click the streaming-taxi-pipeline, or the new job name.

    3. Click Stop, and then select Cancel > Stop Job.



***
Task 7. Create a real-time dashboard

In this task, you create a real-time dashboard to visualize the data.

    1. In the Cloud console, in the Navigation menu (Navigation Menu), click BigQuery.

    3. In the Explorer Pane, expand your Project ID.

    4. Expand Saved queries, and then click My Saved Query.

Your query is loaded in to the query editor.

    4. Click Run.

    5. In BigQuery, click Explore Data > Explore with Looker Studio.

    Looker Studio Opens.

    6. In the Looker Studio window, click your bar chart.

    The Chart pane appears.

    7. Click Add a chart, and then select Combo chart.

    8. In the Setup pane, in Data Range Dimension, hover over minute (Date) and click X to remove it.

    9. In the Data pane, click dashboard_sort and drag it to Setup > Data Range Dimension > Add dimension.

    10. In Setup > Dimension, click minute, and then select dashboard_sort.

    11. In Setup > Metric, click dashboard_sort, and then select total_rides.

    12. In Setup > Metric, click Add metric, and then select total_passengers.

    13. In Setup > Metric, click Record Count, and then select total_revenue.

    14. In Setup > Sort, click total_rides, and then select dashboard_sort.

    15. In Setup > Sort, click Ascending.

Your chart should look similar to this:

    16. When you're happy with your dashboard, click Save and share to save this data source.

    17. If prompted to complete your account setup, agree to the terms and conditions, and then click Continue.

    18. If prompted for email preferences, answer no to all, then click Continue.

    19. If prompted with the Review data access before saving window, click Acknowledge and save.

    20. Click Add to report.

    21. Whenever anyone visits your dashboard, it will be up-to-date with the latest transactions. You can try it yourself by clicking More options (More Options), and then Refresh data.





***
Task 8. Create a time series dashboard

In this task, you create a time series chart.

    1. Click this Looker Studio link to open Looker Studio in a new browser tab.

    2. On the Reports page, in the Start with a Template section, click the [+] Blank Report template.

    3. A new, empty report opens with the Add data to report window.

    4. From the list of Google Connectors, select the BigQuery tile.

    5. Click Custom Query, and then select your ProjectID. This should appear in the following format, qwiklabs-gcp-xxxxxxx.

    6. In Enter Custom Query, paste the following query:
    ```
    SELECT
  *
FROM
  taxirides.realtime
WHERE
  ride_status='dropoff'
  ```

  7. Click Add > Add To Report.

     A new untitled report appears. It may take up to a minute for the screen to finish refreshing.


Create a time series chart

    1. In the Data pane, click Add a Field.

    2. Click All Fields on the left corner.

    3. Change the timestamp field type to Date & Time > Date Hour Minute (YYYYMMDDhhmm).

    4. In the change timestamp dialog, click Continue, and then click Done.

    5. In the top menu, click Add a chart.

    6. Choose Time series chart.

    7. Time Series

    8. Position the chart in the bottom left corner - in the blank space.

    9. In Setup > Dimension, click timestamp (Date), and then select timestamp.

    10. In Setup > Dimension, click timestamp, and then select calendar. Calendar

    11. In Type, select Date & Time > Date Hour Minute.

    12. Click outside the dialog to close it. You do not need to add a name.

    13. In Setup > Metric, click Record Count, and then select meter reading.

Your time series chart should look similar to this but will vary based upon the volume and rate of data that was loaded from Pub/Sub:

Sample time series chart


